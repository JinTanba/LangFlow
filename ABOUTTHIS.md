# Dynamic Flow Orchestrator (MVP) — Engineering Specification

**Stack**: Node.js, TypeScript, Prisma, MongoDB Atlas
**Architecture**: 2 layers (Application, Infrastructure). “Domain” is the Prisma-generated client only.
**Goal**: Execute developer-defined flows (DAGs) where each node is defined inline in the flow document. Runtime “edge LLM” decides which ready nodes to run next and maps outputs to the typed inputs of subsequent nodes. The system natively supports human-in-the-loop pauses via a `human` node.

---

## 1) Scope and Non‑Goals

### In scope (MVP)

* Flow definition stored as JSON (nodes inline, execution order via `requires`).
* Three node kinds:

  * `program`: calls external HTTP endpoints.
  * `ai`: calls an LLM and expects JSON output.
  * `human`: pauses the run, collects human input, then resumes.
* Context persisted in DB and updated after every node completion; external clients can read run status at any time.
* Node I/O defined by **formal JSON Schemas** (stored in the flow).
  Runtime does not perform heavy schema validation. The edge LLM is required to produce inputs conforming to these schemas. If obviously malformed, the engine attempts one corrective re-ask, otherwise the run fails.
* Single orchestrator loop and a worker pool abstraction (actual queue implementation is left to infrastructure).

### Out of scope (MVP)

* Notifications (email/Slack).
* Advanced validation frameworks and static pre-compilation of adapters.
* Versioned tool registries, multi-tenant auth, complex RBAC.
* Cost controls, detailed telemetry dashboards (simple logs are fine).

---

## 2) High-Level Architecture

```
Clients ──▶ HTTP API (read/write) ──▶ Application Layer
                                             │
                                             ▼
                                 Orchestrator (state machine)
                                  │                 │
                                  │                 └─▶ Edge LLM (runtime decisions)
                                  │
                                  └─▶ Worker Engine (executes nodes)
                                                │
                                                ▼
                                     MongoDB Atlas (Prisma)
```

* **Application Layer**

  * Orchestrates runs, maintains the state machine, computes the “ready set” of nodes (based on `requires`), calls the edge LLM to decide next work, updates context.
* **Infrastructure Layer**

  * Implements persistence (Prisma/Mongo), HTTP server, queue/worker plumbing, LLM client adapter, and HTTP caller for `program` nodes.
* **Domain(generated by prisma)**
    watch --> /Users/tanbajintaro/Development/softsoftware/prisma/schema.prisma

---

## 3) Core Concepts

### Flow

A single JSON document representing a DAG where each node is defined inline and references formal JSON Schemas for its input and output.

### Node (kinds)

* **program**: invokes an HTTP endpoint.
* **ai**: invokes an LLM with an instruction; expects JSON adhering to `output_schema`.
* **human**: pauses execution until a human submits a form; the submitted JSON becomes the node output.

### Requires (order/joins)

* `requires: string[]` on a node means “all listed nodes must be finished (`ok` or `skipped`) before this node becomes *ready*.”
* Parallelism arises naturally when multiple ready nodes exist.
* Conditionals are handled at runtime by edge LLM (by selecting a subset of ready nodes to run and optionally marking some as `skipped`).

### Context (persistent)

* Stored in `FlowRun.context` as flexible JSON:

  * `vars`: shared scratch-space updated by nodes/edge LLM if needed.
  * `node_results`: `{ [nodeKey]: { status, output?, error?, finishedAt? } }`.
  * Timestamps and any other view-oriented fragments as needed.
* This context is the primary source of truth for external status queries.

### Edge LLM (runtime only)

* Given the ready nodes, last output, the flow’s JSON schemas, and the current context, it returns:

  * `mode`: `next | parallel | stop`
  * `next`: array of `{ nodeKey, input }` (and optional `human_hint` for human nodes)
  * `skips`: array of ready nodes to mark as `skipped`
  * `reason`: brief explanation for auditing
* JSON-only contract. One correction attempt on malformed responses; otherwise the run fails.

---

## 4) Execution Lifecycle

1. **Start run**

   * Create `FlowRun` with `status=running`, persist initial `context` (`vars`, empty `node_results`).
2. **Compute ready set**

   * Nodes not yet finished for which `requires` are all `ok` or `skipped`.
3. **Decision step (edge LLM)**

   * Orchestrator sends: flow meta, ready nodes + their `input_schema`, last completed node’s output, and context.
   * LLM returns `next` and optional `skips`.

     * If `stop` and no ready nodes remain, mark run `completed`.
4. **Dispatch**

   * For each item in `next`:

     * If node.kind = `program` → enqueue/run HTTP call.
     * If node.kind = `ai` → enqueue/run LLM call (expects JSON).
     * If node.kind = `human` → create `HumanTask` (pending); mark `NodeRun` as `waiting_human`.

       * If `blocking=true`, set `FlowRun.status=waiting`; otherwise keep running.
5. **Completion**

   * On node finish: write `NodeRun` record, update `context.node_results[nodeKey]`, possibly update `vars`, set `FlowRun.status=running` if it was waiting and the blocking task just completed.
   * Append a `DecisionLog` record after each decision step.
6. **Termination**

   * When the decision step yields `stop` and no more ready nodes exist, set `FlowRun.status=completed`. On fatal error, set `failed`.

**Skip semantics**: A node can be marked `skipped` by the edge LLM in the decision step. Skipped nodes count as satisfied for downstream `requires`.

---

## 5) Data Model (logical)

> The following is the logical model and field intent. The exact Prisma schema is already defined separately; this section is for understanding and implementation alignment.

### Flow

* `id`, `name`, `version`
* `nodes: Node[]` (inline)

  * `key` (string, unique within flow)
  * `kind` (`program | ai | human`)
  * `title?`, `description?`
  * `requires?: string[]` (predecessor keys)
  * `input_schema: JSON` (JSON Schema 2020-12)
  * `output_schema: JSON`
  * For `program`:

    * `endpoint: { method, url, headers?, body_template? }`
  * For `ai`:

    * `model`, `system?`, `format="json"`
  * For `human`:

    * `blocking?: boolean` (default true)
    * `assignees?: string[]` (MVP: stored only; no notifications)
    * `timeout_sec?: number`
    * `ui_hint?: { message?: string; fields?: any[] }` (edge LLM may refine)

### FlowRun

* `id`, `flowId`, `status` (`queued | running | waiting | completed | failed`)
* `input: JSON`
* `context: JSON`

  * `vars: JSON object`
  * `node_results: { [nodeKey]: { status, output?, error?, finishedAt? } }`
  * `started_at`, `updated_at`
* Timestamps

### NodeRun

* `id`, `flowRunId`, `nodeKey`, `nodeType` (`program | ai | human`)
* `status` (`queued | running | ok | error | skipped | waiting_human`)
* `input: JSON`, `output?: JSON`, `error?: JSON`
* `startedAt?`, `finishedAt?`

### HumanTask

* `id`, `flowRunId`, `nodeKey`
* `status` (`pending | submitted | expired | canceled`)
* `message?`, `fields?`, `prefill?`, `result?`
* `assignees: string[]`
* `blocking: boolean` (default true)
* `token: string` (unique; used to open a simple form)
* `expiresAt?`
* Timestamps

### DecisionLog

* `id`, `flowRunId`, `atNodeKey?`
* `decision: JSON`
  Shape: `{ mode, next: [{nodeKey,input,human?}], skips?: string[], reason?: string }`
* `createdAt`

**Indexes (recommended)**

* `FlowRun`: by `flowId`, and (`status`, `updatedAt`) for dashboards.
* `NodeRun`: by (`flowRunId`, `nodeKey`) and by (`status`, `finishedAt`).
* `HumanTask`: by `flowRunId`, `status`, `expiresAt`.
* `DecisionLog`: by `flowRunId`, `createdAt`.

---

## 6) Flow JSON Contract

### Flow document (illustrative)

```json
{
  "name": "A-then-(B,C)-then-D",
  "version": 1,
  "nodes": [
    {
      "key": "A",
      "kind": "program",
      "title": "User Lookup",
      "requires": [],
      "input_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "phone": { "type": "string" } }, "required": ["phone"] },
      "output_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "userId": { "type": "string" }, "risk": { "type": "object", "properties": { "score": { "type": "number" } }, "required": ["score"] }, "vip": { "type": "boolean" } }, "required": ["userId","risk"] },
      "endpoint": { "method": "POST", "url": "https://svc/users/lookup" }
    },
    {
      "key": "B",
      "kind": "program",
      "requires": ["A"],
      "input_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "userId": { "type": "string" } }, "required": ["userId"] },
      "output_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "verified": { "type": "boolean" } }, "required": ["verified"] },
      "endpoint": { "method": "POST", "url": "https://svc/verify/light" }
    },
    {
      "key": "C",
      "kind": "ai",
      "requires": ["A"],
      "input_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "userId": { "type": "string" }, "reason": { "type": "string" } }, "required": ["userId"] },
      "output_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "reviewScore": { "type": "number" }, "notes": { "type": "string" } }, "required": ["reviewScore"] },
      "model": "gpt-4.1-mini",
      "system": "Return JSON only."
    },
    {
      "key": "H",
      "kind": "human",
      "requires": ["B","C"],
      "input_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "userId": { "type": "string" }, "score": { "type": "number" } }, "required": ["userId"] },
      "output_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "decision": { "type": "string", "enum": ["approve","reject"] }, "note": { "type": "string" } }, "required": ["decision"] },
      "blocking": true,
      "timeout_sec": 3600,
      "ui_hint": { "message": "Approve or reject this user.", "fields": [{ "name": "decision", "type": "select", "options": ["approve","reject"] }, { "name": "note", "type": "textarea" }] }
    },
    {
      "key": "D",
      "kind": "program",
      "requires": ["H"],
      "input_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "userId": { "type": "string" }, "decision": { "type": "string" } }, "required": ["userId","decision"] },
      "output_schema": { "$schema": "https://json-schema.org/draft/2020-12/schema", "type": "object", "properties": { "ok": { "type": "boolean" } }, "required": ["ok"] },
      "endpoint": { "method": "POST", "url": "https://svc/finalize" }
    }
  ]
}
```

---

## 7) Edge LLM Contract

### Input (to LLM)

```json
{
  "flow": { "name": "A-then-(B,C)-then-D", "version": 1 },
  "ready_nodes": [
    { "key": "B", "kind": "program", "input_schema": { /* JSON Schema */ } },
    { "key": "C", "kind": "ai",      "input_schema": { /* JSON Schema */ } }
  ],
  "context": {
    "input": { "phone": "+81..." },
    "vars": {},
    "completed": [
      { "key": "A", "output": { "userId": "u123", "risk": { "score": 0.9 }, "vip": false } }
    ]
  },
  "last": { "key": "A", "output": { "userId": "u123", "risk": { "score": 0.9 }, "vip": false } }
}
```

### Output (from LLM)

```json
{
  "mode": "next",
  "next": [
    { "nodeKey": "C", "input": { "userId": "u123", "reason": "risk.high" } }
  ],
  "skips": ["B"],
  "reason": "risk.score>=0.8; run C, skip B"
}
```

**Human node example**

```json
{
  "mode": "next",
  "next": [
    {
      "nodeKey": "H",
      "input": { "userId": "u123", "score": 0.92 },
      "human": {
        "message": "High risk case. Please approve/reject.",
        "fields": [
          { "name": "decision", "type": "select", "options": ["approve","reject"], "required": true },
          { "name": "note", "type": "textarea" }
        ]
      }
    }
  ],
  "reason": "B or C completed; collect human decision before D"
}
```

**Failure policy**

* Non-JSON or structurally invalid response → one immediate correction request → if still invalid, mark run `failed`.

---

## 8) Minimal API Surface

> Authentication/authorization is out of scope for MVP. Assume trusted internal callers.

* **POST `/flows`**
  Create/update a flow document.
* **GET `/flows/{id}`**
  Retrieve a flow document.
* **POST `/flows/{id}/runs`**
  Start a new run with an initial `input`.
* **GET `/runs/{runId}`**
  Retrieve run status including the full `context` and summary of `node_runs`.
* **GET `/runs/{runId}/human-tasks`**
  List pending human tasks for a run.
* **GET `/human-tasks/{token}`**
  Retrieve a single human task by token (used by a lightweight UI).
* **POST `/human-tasks/{token}/submit`**
  Submit result JSON; completes the human node and resumes the run.
* **GET `/runs/{runId}/decisions`**
  (Optional) Retrieve decision logs for auditing.

---

## 9) Concurrency & Scheduling

* A node becomes **ready** when all `requires` are `ok` or `skipped`.
* The orchestrator asks the edge LLM to pick from the ready set.
* The worker engine may execute multiple nodes concurrently when `next` contains several entries.
* `human` nodes:

  * When created with `blocking=true`, the run status transitions to `waiting` and the orchestrator pauses further scheduling until submission. If `blocking=false`, other branches can continue.

---

## 10) Error Handling (MVP)

* **Program node**: HTTP/network error → mark node `error` and run `failed`.
* **AI node**: non-JSON or malformed → one re-ask; if still malformed → mark node `error` and run `failed`.
* **Edge LLM**: invalid decision JSON → one re-ask; else run `failed`.
* **Human task**: if expired → mark task `expired`, mark node `error` (or `skipped` if you choose to treat timeout that way in a later iteration), then run `failed` for MVP.

---

## 11) Persistence & Observability (MVP)

* Persist all state transitions (`FlowRun`, `NodeRun`, `HumanTask`) immediately to MongoDB.
* Log each decision (`DecisionLog`) for auditability (no fancy dashboard required).
* Add minimal indexes:

  * `FlowRun`: by `flowId`, and (`status`, `updatedAt`)
  * `NodeRun`: by (`flowRunId`, `nodeKey`)
  * `HumanTask`: by `status`, `expiresAt`

---

## 12) Project Structure (coarse)

```
.
├─ prisma/               # Prisma schema & migrations
├─ src/
│  ├─ application/               # Application layer (orchestrator, use cases, ports)
│  ├─ infrastructure/             # Infrastructure (HTTP server, persistence, queue, LLM adapter)
│  └─ generated/            # Prisma-generated client (no handwritten code)
└─ .env
```

No per-file decisions in MVP. The application layer depends only on ports; infrastructure provides adapters.

---

## 13) Security & Privacy (MVP)

* Assume flows and runs are internal.
* Store only necessary data in `context`. If PII is present, keep it limited to what nodes strictly require.
* TLS for all external HTTP calls.
* Secrets via environment variables.

---

## 14) Open Questions (post‑MVP)

* Should timeouts on `human` nodes auto-approve, auto-reject, or fail?
* Introduce light schema validation on engine side (fast fail) without going full AJV?
* Introduce a durable queue for workers for better fault tolerance?
* Support partial retries or compensations (SAGA) for program nodes?

---

## 15) Acceptance Criteria

* You can create a flow with inline nodes and formal JSON Schemas.
* You can start a run, observe live status via `GET /runs/{runId}`, and see `context` evolve.
* The edge LLM can:

  * Choose among ready nodes,
  * Mark nodes as `skipped`,
  * Produce typed `input` JSON for selected nodes,
  * Emit a `human` node with form hints that pauses and resumes the run.
* A complete happy-path run finishes with `status=completed`. Any unrecoverable error yields `failed`.
* No notifications are required.

---

This specification defines **what** to build for the MVP: a two-layer, schema-aware runtime where edge LLM orchestrates dynamic branching and input mapping, context is always persisted for external visibility, and human approval is a first-class node that pauses and resumes the flow.






├─ src/
│  ├─ application/                # ここだけが“アプリケーション”
│  │  ├─ usecases/                # 入出力を受けて一件の仕事を完遂する“手続き”
│  │  │  ├─ StartFlowRun/
│  │  │  ├─ DecideNext/          # “準備OKノード集合”→ edgeLLM に問い合わせ → 次を出す
│  │  │  ├─ RunNode/             # Program / AI / Human を起動（サービスに委譲）
│  │  │  ├─ SubmitHumanTask/     # 人間入力を受けて再開
│  │  │  └─ GetRunStatus/        # Contextを読み出すだけ（外部可視化）
│  │  ├─ services/               # 状態遷移や変換ロジック、外部I/Oの抽象
│  │  │  ├─ Orchestrator/        # “requires”充足・準備集合・停止条件などの決定論ロジック
│  │  │  ├─ EdgeDecision/        # edgeLLMとの対話契約（JSON入出力の整形だけ）
│  │  │  └─ Runners/             # Program/Ai/Human の実行サービス（I/Fだけ定義）
│  │  └─ repositories/           # Flow/FlowRun/NodeRun/HumanTask/DecisionLog の抽象I/F
